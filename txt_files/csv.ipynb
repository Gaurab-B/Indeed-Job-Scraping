{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5769b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3bb0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_from_file(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "            content = file.read()\n",
    "            return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1424242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_details(soup):\n",
    "    jobs = soup.find_all('div', class_='job_seen_beacon')\n",
    "    job_titles, companies, locations, salaries, job_types, descriptions, dates, links = [], [], [], [], [], [], [], []\n",
    "\n",
    "    # Loop through each job listing\n",
    "    for job in jobs:\n",
    "        # Get job titles\n",
    "        title_elements = job.find_all('h2', class_=['jobTitle css-198pbd eu4oa1w0', 'jobTitle jobTitle-newJob css-198pbd eu4oa1w0'])\n",
    "        for h2 in title_elements:\n",
    "            job_name_span = h2.find('span', id=lambda x: x and x.startswith('jobTitle-'))\n",
    "            if job_name_span:\n",
    "                job_name = job_name_span.get('title', job_name_span.text)\n",
    "                job_titles.append(job_name)\n",
    "\n",
    "        # Get company and location details\n",
    "        company_div = job.find('span', {'data-testid': 'company-name'})\n",
    "        location_div = job.find('div', {'data-testid': 'text-location'})\n",
    "        if company_div:\n",
    "            companies.append(company_div.text)\n",
    "        if location_div:\n",
    "            locations.append(location_div.text)\n",
    "\n",
    "        # Get salary and job type details\n",
    "        pay_div = job.find('div', class_='salary-snippet-container')\n",
    "        type_div = job.find('div', class_='metadata css-5zy3wz eu4oa1w0')\n",
    "        salaries.append(pay_div.text.strip() if pay_div else 'Not Provided')\n",
    "        job_types.append(type_div.find('div', {'data-testid': 'attribute_snippet_testid'}).text.strip() if type_div else 'Not Provided')\n",
    "\n",
    "        # Get job description\n",
    "        descr = job.find('div', class_='css-1u8dvic eu4oa1w0')\n",
    "        if descr:\n",
    "            descriptions.append(descr.find('ul').text.strip() if descr.find('ul') else '')\n",
    "        \n",
    "        # Get job posting date\n",
    "        date_element = job.find('span', class_='css-qvloho eu4oa1w0')\n",
    "        if date_element:\n",
    "            dates.append(date_element.text.strip())\n",
    "\n",
    "        # Get job link\n",
    "        a_tag = job.find('h2', class_='jobTitle').find('a') if job.find('h2', class_='jobTitle') else None\n",
    "        if a_tag:\n",
    "            links.append(\"https://www.indeed.com\" + a_tag.get('href', ''))\n",
    "\n",
    "    return job_titles, companies, locations, salaries, job_types, descriptions, dates, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1796e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(dataframe, file_number):\n",
    "    output_text = f\"Cleaned_csvs/AI-UnitedStates-Page({file_number}).csv\"\n",
    "    dataframe.to_csv(output_text, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "972ee0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_folder(folder_path):\n",
    "    # Get a list of all .txt files in the folder\n",
    "    file_list = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            content = read_html_from_file(file_path)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            job_titles, companies, locations, salaries, job_types, descriptions, dates, links = parse_job_details(soup)\n",
    "            file_number = file_name.split('_')[1].split('.')[0]\n",
    "            df = pd.DataFrame({\n",
    "            'Job Title': job_titles,\n",
    "            'Company': companies,\n",
    "            'Location': locations,\n",
    "            'Salary($)': salaries,\n",
    "            'Job-Type': job_types,\n",
    "            'Job-Description': descriptions,\n",
    "            'Raw_Link': links})\n",
    "            csv_file_name = f'Cleaned_csvs/page_{file_number}.csv'\n",
    "            df.to_csv(csv_file_name, index=False)\n",
    "            print(f'Saved {csv_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efbbfd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Cleaned_csvs/page_1.csv\n",
      "Saved Cleaned_csvs/page_10.csv\n",
      "Saved Cleaned_csvs/page_100.csv\n",
      "Saved Cleaned_csvs/page_101.csv\n",
      "Saved Cleaned_csvs/page_11.csv\n",
      "Saved Cleaned_csvs/page_12.csv\n",
      "Saved Cleaned_csvs/page_13.csv\n",
      "Saved Cleaned_csvs/page_14.csv\n",
      "Saved Cleaned_csvs/page_15.csv\n",
      "Saved Cleaned_csvs/page_16.csv\n",
      "Saved Cleaned_csvs/page_17.csv\n",
      "Saved Cleaned_csvs/page_18.csv\n",
      "Saved Cleaned_csvs/page_19.csv\n",
      "Saved Cleaned_csvs/page_2.csv\n",
      "Saved Cleaned_csvs/page_20.csv\n",
      "Saved Cleaned_csvs/page_21.csv\n",
      "Saved Cleaned_csvs/page_22.csv\n",
      "Saved Cleaned_csvs/page_23.csv\n",
      "Saved Cleaned_csvs/page_24.csv\n",
      "Saved Cleaned_csvs/page_25.csv\n",
      "Saved Cleaned_csvs/page_26.csv\n",
      "Saved Cleaned_csvs/page_27.csv\n",
      "Saved Cleaned_csvs/page_28.csv\n",
      "Saved Cleaned_csvs/page_29.csv\n",
      "Saved Cleaned_csvs/page_3.csv\n",
      "Saved Cleaned_csvs/page_30.csv\n",
      "Saved Cleaned_csvs/page_31.csv\n",
      "Saved Cleaned_csvs/page_32.csv\n",
      "Saved Cleaned_csvs/page_33.csv\n",
      "Saved Cleaned_csvs/page_34.csv\n",
      "Saved Cleaned_csvs/page_35.csv\n",
      "Saved Cleaned_csvs/page_36.csv\n",
      "Saved Cleaned_csvs/page_37.csv\n",
      "Saved Cleaned_csvs/page_38.csv\n",
      "Saved Cleaned_csvs/page_39.csv\n",
      "Saved Cleaned_csvs/page_4.csv\n",
      "Saved Cleaned_csvs/page_40.csv\n",
      "Saved Cleaned_csvs/page_41.csv\n",
      "Saved Cleaned_csvs/page_42.csv\n",
      "Saved Cleaned_csvs/page_43.csv\n",
      "Saved Cleaned_csvs/page_44.csv\n",
      "Saved Cleaned_csvs/page_45.csv\n",
      "Saved Cleaned_csvs/page_46.csv\n",
      "Saved Cleaned_csvs/page_47.csv\n",
      "Saved Cleaned_csvs/page_48.csv\n",
      "Saved Cleaned_csvs/page_49.csv\n",
      "Saved Cleaned_csvs/page_5.csv\n",
      "Saved Cleaned_csvs/page_50.csv\n",
      "Saved Cleaned_csvs/page_51.csv\n",
      "Saved Cleaned_csvs/page_52.csv\n",
      "Saved Cleaned_csvs/page_53.csv\n",
      "Saved Cleaned_csvs/page_54.csv\n",
      "Saved Cleaned_csvs/page_55.csv\n",
      "Saved Cleaned_csvs/page_56.csv\n",
      "Saved Cleaned_csvs/page_57.csv\n",
      "Saved Cleaned_csvs/page_58.csv\n",
      "Saved Cleaned_csvs/page_59.csv\n",
      "Saved Cleaned_csvs/page_6.csv\n",
      "Saved Cleaned_csvs/page_60.csv\n",
      "Saved Cleaned_csvs/page_61.csv\n",
      "Saved Cleaned_csvs/page_62.csv\n",
      "Saved Cleaned_csvs/page_63.csv\n",
      "Saved Cleaned_csvs/page_64.csv\n",
      "Saved Cleaned_csvs/page_65.csv\n",
      "Saved Cleaned_csvs/page_66.csv\n",
      "Saved Cleaned_csvs/page_67.csv\n",
      "Saved Cleaned_csvs/page_68.csv\n",
      "Saved Cleaned_csvs/page_69.csv\n",
      "Saved Cleaned_csvs/page_7.csv\n",
      "Saved Cleaned_csvs/page_70.csv\n",
      "Saved Cleaned_csvs/page_71.csv\n",
      "Saved Cleaned_csvs/page_72.csv\n",
      "Saved Cleaned_csvs/page_73.csv\n",
      "Saved Cleaned_csvs/page_74.csv\n",
      "Saved Cleaned_csvs/page_75.csv\n",
      "Saved Cleaned_csvs/page_76.csv\n",
      "Saved Cleaned_csvs/page_77.csv\n",
      "Saved Cleaned_csvs/page_78.csv\n",
      "Saved Cleaned_csvs/page_79.csv\n",
      "Saved Cleaned_csvs/page_8.csv\n",
      "Saved Cleaned_csvs/page_80.csv\n",
      "Saved Cleaned_csvs/page_81.csv\n",
      "Saved Cleaned_csvs/page_82.csv\n",
      "Saved Cleaned_csvs/page_83.csv\n",
      "Saved Cleaned_csvs/page_84.csv\n",
      "Saved Cleaned_csvs/page_85.csv\n",
      "Saved Cleaned_csvs/page_86.csv\n",
      "Saved Cleaned_csvs/page_87.csv\n",
      "Saved Cleaned_csvs/page_88.csv\n",
      "Saved Cleaned_csvs/page_89.csv\n",
      "Saved Cleaned_csvs/page_9.csv\n",
      "Saved Cleaned_csvs/page_90.csv\n",
      "Saved Cleaned_csvs/page_91.csv\n",
      "Saved Cleaned_csvs/page_92.csv\n",
      "Saved Cleaned_csvs/page_93.csv\n",
      "Saved Cleaned_csvs/page_94.csv\n",
      "Saved Cleaned_csvs/page_95.csv\n",
      "Saved Cleaned_csvs/page_96.csv\n",
      "Saved Cleaned_csvs/page_97.csv\n",
      "Saved Cleaned_csvs/page_98.csv\n",
      "Saved Cleaned_csvs/page_99.csv\n"
     ]
    }
   ],
   "source": [
    "folder_path = '.'\n",
    "process_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929cbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
